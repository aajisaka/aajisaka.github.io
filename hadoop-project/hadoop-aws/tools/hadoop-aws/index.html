<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!--
 | Generated by Apache Maven Doxia at 2016-07-12
 | Rendered using Apache Maven Stylus Skin 1.5
-->
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>Apache Hadoop Amazon Web Services support &#x2013; Hadoop-AWS module: Integration with Amazon Web Services</title>
    <style type="text/css" media="all">
      @import url("../../css/maven-base.css");
      @import url("../../css/maven-theme.css");
      @import url("../../css/site.css");
    </style>
    <link rel="stylesheet" href="../../css/print.css" type="text/css" media="print" />
        <meta name="Date-Revision-yyyymmdd" content="20160712" />
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
                </head>
  <body class="composite">
    <div id="banner">
                        <a href="http://hadoop.apache.org/" id="bannerLeft">
                                        <img src="http://hadoop.apache.org/images/hadoop-logo.jpg" alt="" />
                </a>
                              <a href="http://www.apache.org/" id="bannerRight">
                                        <img src="http://www.apache.org/images/asf_logo_wide.png" alt="" />
                </a>
            <div class="clear">
        <hr/>
      </div>
    </div>
    <div id="breadcrumbs">
            
                                   <div class="xleft">
                          <a href="http://www.apache.org/" class="externalLink">Apache</a>
        &gt;
                  <a href="http://hadoop.apache.org/" class="externalLink">Hadoop</a>
        &gt;
                  <a href="../../index.html">Apache Hadoop Amazon Web Services support</a>
        &gt;
        Hadoop-AWS module: Integration with Amazon Web Services
        </div>
            <div class="xright">            <a href="http://wiki.apache.org/hadoop" class="externalLink">Wiki</a>
            |
                <a href="https://git-wip-us.apache.org/repos/asf/hadoop.git" class="externalLink">git</a>
              
                                   &nbsp;| Last Published: 2016-07-12
              &nbsp;| Version: 3.0.0-alpha1-SNAPSHOT
            </div>
      <div class="clear">
        <hr/>
      </div>
    </div>
    <div id="leftColumn">
      <div id="navcolumn">
             
                                                   <h5>General</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../index.html">Overview</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/SingleCluster.html">Single Node Setup</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/ClusterSetup.html">Cluster Setup</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/CommandsManual.html">Commands Reference</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/FileSystemShell.html">FileSystem Shell</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/Compatibility.html">Compatibility</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/InterfaceClassification.html">Interface Classification</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/filesystem/index.html">FileSystem Specification</a>
            </li>
          </ul>
                       <h5>Common</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/CLIMiniCluster.html">CLI Mini Cluster</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/NativeLibraries.html">Native Libraries</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/Superusers.html">Proxy User</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/RackAwareness.html">Rack Awareness</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/SecureMode.html">Secure Mode</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/ServiceLevelAuth.html">Service Level Authorization</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/HttpAuthentication.html">HTTP Authentication</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/CredentialProviderAPI.html">Credential Provider API</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-kms/index.html">Hadoop KMS</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/Tracing.html">Tracing</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/UnixShellGuide.html">Unix Shell Guide</a>
            </li>
          </ul>
                       <h5>HDFS</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsDesign.html">Architecture</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html">User Guide</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HDFSCommands.html">Commands Reference</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html">NameNode HA With QJM</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithNFS.html">NameNode HA With NFS</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/Federation.html">Federation</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/ViewFs.html">ViewFs</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsSnapshots.html">Snapshots</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsEditsViewer.html">Edits Viewer</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsImageViewer.html">Image Viewer</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.html">Permissions and HDFS</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsQuotaAdminGuide.html">Quotas and HDFS</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/LibHdfs.html">libhdfs (C API)</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/WebHDFS.html">WebHDFS (REST API)</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-hdfs-httpfs/index.html">HttpFS</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/ShortCircuitLocalReads.html">Short Circuit Local Reads</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/CentralizedCacheManagement.html">Centralized Cache Management</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsNfsGateway.html">NFS Gateway</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsRollingUpgrade.html">Rolling Upgrade</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/ExtendedAttributes.html">Extended Attributes</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/TransparentEncryption.html">Transparent Encryption</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HdfsMultihoming.html">Multihoming</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/ArchivalStorage.html">Storage Policies</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/MemoryStorage.html">Memory Storage Support</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/SLGUserGuide.html">Synthetic Load Generator</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/HDFSErasureCoding.html">Erasure Coding</a>
            </li>
          </ul>
                       <h5>MapReduce</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html">Tutorial</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapredCommands.html">Commands Reference</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduce_Compatibility_Hadoop1_Hadoop2.html">Compatibility with 1.x</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/EncryptedShuffle.html">Encrypted Shuffle</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/PluggableShuffleAndPluggableSort.html">Pluggable Shuffle/Sort</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/DistributedCacheDeploy.html">Distributed Cache Deploy</a>
            </li>
          </ul>
                       <h5>MapReduce REST APIs</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapredAppMasterRest.html">MR Application Master</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-hs/HistoryServerRest.html">MR History Server</a>
            </li>
          </ul>
                       <h5>YARN</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/YARN.html">Architecture</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/YarnCommands.html">Commands Reference</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html">Capacity Scheduler</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/FairScheduler.html">Fair Scheduler</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/ResourceManagerRestart.html">ResourceManager Restart</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html">ResourceManager HA</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/NodeLabel.html">Node Labels</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/WebApplicationProxy.html">Web Application Proxy</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/TimelineServer.html">Timeline Server</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/TimelineServiceV2.html">Timeline Service V.2</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/WritingYarnApplications.html">Writing YARN Applications</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/YarnApplicationSecurity.html">YARN Application Security</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/NodeManager.html">NodeManager</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/DockerContainerExecutor.html">DockerContainerExecutor</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/NodeManagerCgroups.html">Using CGroups</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/SecureContainer.html">Secure Containers</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/registry/index.html">Registry</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/ReservationSystem.html">Reservation System</a>
            </li>
          </ul>
                       <h5>YARN REST APIs</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/WebServicesIntro.html">Introduction</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/ResourceManagerRest.html">Resource Manager</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/NodeManagerRest.html">Node Manager</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/TimelineServer.html#Timeline_Server_REST_API_v1">Timeline Server</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-site/TimelineServiceV2.html#Timeline_Service_REST_API_v2">Timeline Service V.2</a>
            </li>
          </ul>
                       <h5>Hadoop Compatible File Systems</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-aws/tools/hadoop-aws/index.html">Amazon S3</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-azure/index.html">Azure Blob Storage</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-azure-datalake/index.html">Azure Data Lake Storage</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-openstack/index.html">OpenStack Swift</a>
            </li>
          </ul>
                       <h5>Auth</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-auth/index.html">Overview</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-auth/Examples.html">Examples</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-auth/Configuration.html">Configuration</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-auth/BuildingIt.html">Building</a>
            </li>
          </ul>
                       <h5>Tools</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-streaming/HadoopStreaming.html">Hadoop Streaming</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-archives/HadoopArchives.html">Hadoop Archives</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-archive-logs/HadoopArchiveLogs.html">Hadoop Archive Logs</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-distcp/DistCp.html">DistCp</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-gridmix/GridMix.html">GridMix</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-rumen/Rumen.html">Rumen</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-sls/SchedulerLoadSimulator.html">Scheduler Load Simulator</a>
            </li>
          </ul>
                       <h5>Reference</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/release/index.html">Changelog and Release Notes</a>
            </li>
                  <li class="none">
                  <a href="../../../api/index.html">Java API docs</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/UnixShellAPI.html">Unix Shell API</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/Metrics.html">Metrics</a>
            </li>
          </ul>
                       <h5>Configuration</h5>
                  <ul>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/core-default.xml">core-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-hdfs/hdfs-default.xml">hdfs-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml">mapred-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-yarn/hadoop-yarn-common/yarn-default.xml">yarn-default.xml</a>
            </li>
                  <li class="none">
                  <a href="../../../hadoop-project-dist/hadoop-common/DeprecatedProperties.html">Deprecated Properties</a>
            </li>
          </ul>
                                 <a href="http://maven.apache.org/" title="Built by Maven" class="poweredBy">
          <img alt="Built by Maven" src="../../images/logos/maven-feather.png"/>
        </a>
                       
                               </div>
    </div>
    <div id="bodyColumn">
      <div id="contentBox">
        <!---
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
--><h1>Hadoop-AWS module: Integration with Amazon Web Services</h1>

<ul>
<li><a href="#Overview">Overview</a>
<ul>
<li><a href="#Features">Features</a></li>
<li><a href="#Warning_1:_Object_Stores_are_not_filesystems.">Warning #1: Object Stores are not filesystems.</a></li>
<li><a href="#Warning_2:_Because_Object_stores_dont_track_modification_times_of_directories">Warning #2: Because Object stores don&#x2019;t track modification times of directories,</a></li>
<li><a href="#Warning_3:_your_AWS_credentials_are_valuable">Warning #3: your AWS credentials are valuable</a></li>
<li><a href="#Warning_4:_the_S3_client_provided_by_Amazon_EMR_are_not_from_the_Apache">Warning #4: the S3 client provided by Amazon EMR are not from the Apache</a></li></ul></li>
<li><a href="#S3">S3</a>
<ul>
<li><a href="#Dependencies">Dependencies</a></li>
<li><a href="#Authentication_properties">Authentication properties</a></li></ul></li>
<li><a href="#S3N">S3N</a>
<ul>
<li><a href="#Features">Features</a></li>
<li><a href="#Dependencies">Dependencies</a></li>
<li><a href="#Authentication_properties">Authentication properties</a></li>
<li><a href="#Other_properties">Other properties</a></li></ul></li>
<li><a href="#S3A">S3A</a>
<ul>
<li><a href="#Features">Features</a></li>
<li><a href="#Dependencies">Dependencies</a></li>
<li><a href="#Authentication_properties">Authentication properties</a>
<ul>
<li><a href="#Authentication_methods">Authentication methods</a></li>
<li><a href="#Protecting_the_AWS_Credentials_in_S3A">Protecting the AWS Credentials in S3A</a></li>
<li><a href="#Authenticating_via_environment_variables">Authenticating via environment variables</a>
<ul>
<li><a href="#End_to_End_Steps_for_Distcp_and_S3_with_Credential_Providers">End to End Steps for Distcp and S3 with Credential Providers</a>
<ul>
<li><a href="#provision">provision</a></li>
<li><a href="#configure_core-site.xml_or_command_line_system_property">configure core-site.xml or command line system property</a></li>
<li><a href="#distcp">distcp</a></li></ul></li></ul></li></ul></li>
<li><a href="#Other_properties">Other properties</a></li>
<li><a href="#S3AFastOutputStream">S3AFastOutputStream</a></li>
<li><a href="#S3A_Experimental_fadvise_input_policy_support">S3A Experimental &#x201c;fadvise&#x201d; input policy support</a>
<ul>
<li><a href="#asequential_default">&#x201c;sequential&#x201d; (default)</a></li>
<li><a href="#anormal">&#x201c;normal&#x201d;</a></li>
<li><a href="#arandom">&#x201c;random&#x201d;</a></li></ul></li></ul></li>
<li><a href="#Troubleshooting_S3A">Troubleshooting S3A</a>
<ul>
<li><a href="#ClassNotFoundException:_org.apache.hadoop.fs.s3a.S3AFileSystem">ClassNotFoundException: org.apache.hadoop.fs.s3a.S3AFileSystem</a></li>
<li><a href="#ClassNotFoundException:_com.amazonaws.services.s3.AmazonS3Client">ClassNotFoundException: com.amazonaws.services.s3.AmazonS3Client</a></li>
<li><a href="#Missing_method_in_AWS_class">Missing method in AWS class</a></li>
<li><a href="#Missing_method_in_a_Jackson_class">Missing method in a Jackson class</a></li>
<li><a href="#Authentication_failure">Authentication failure</a></li>
<li><a href="#Authentication_failures_running_on_Java_8u60">Authentication failures running on Java 8u60+</a></li></ul></li>
<li><a href="#Visible_S3_Inconsistency">Visible S3 Inconsistency</a>
<ul>
<li><a href="#FileNotFoundException_even_though_the_file_was_just_written.">FileNotFoundException even though the file was just written.</a></li>
<li><a href="#File_not_found_in_a_directory_listing_even_though_getFileStatus_finds_it">File not found in a directory listing, even though getFileStatus() finds it</a></li>
<li><a href="#File_not_visiblesaved">File not visible/saved</a></li>
<li><a href="#File_flush_and_hflush_calls_do_not_save_data_to_S3A">File flush() and hflush() calls do not save data to S3A</a></li>
<li><a href="#Other_issues">Other issues</a></li></ul></li>
<li><a href="#Testing_the_S3_filesystem_clients">Testing the S3 filesystem clients</a>
<ul>
<li><a href="#core-site.xml">core-site.xml</a></li>
<li><a href="#auth-keys.xml">auth-keys.xml</a></li>
<li><a href="#File_contract-test-options.xml">File contract-test-options.xml</a></li>
<li><a href="#s3n:">s3n://</a></li>
<li><a href="#s3a:">s3a://</a></li>
<li><a href="#Complete_example_of_contract-test-options.xml">Complete example of contract-test-options.xml</a></li>
<li><a href="#Running_Tests_against_non-AWS_storage_infrastructures">Running Tests against non-AWS storage infrastructures</a></li>
<li><a href="#S3A_session_tests">S3A session tests</a>
<ul>
<li><a href="#CSV_Data_source">CSV Data source</a></li>
<li><a href="#Scale_test_operation_count">Scale test operation count</a></li></ul></li>
<li><a href="#Running_the_Tests">Running the Tests</a></li>
<li><a href="#Testing_against_non_AWS_S3_endpoints.">Testing against non AWS S3 endpoints.</a></li></ul></li></ul>
<div class="section">
<h2><a name="Overview"></a>Overview</h2>
<p>The <tt>hadoop-aws</tt> module provides support for AWS integration. The generated JAR file, <tt>hadoop-aws.jar</tt> also declares a transitive dependency on all external artifacts which are needed for this support &#x2014;enabling downstream applications to easily use this support.</p>
<p>To make it part of Apache Hadoop&#x2019;s default classpath, simply make sure that HADOOP_OPTIONAL_TOOLS in hadoop-env.sh has &#x2018;hadoop-aws&#x2019; in the list.</p>
<div class="section">
<h3><a name="Features"></a>Features</h3>
<p><b>NOTE: <tt>s3:</tt> is being phased out. Use <tt>s3n:</tt> or <tt>s3a:</tt> instead.</b></p>

<ol style="list-style-type: decimal">
  
<li>The second-generation, <tt>s3n:</tt> filesystem, making it easy to share data between hadoop and other applications via the S3 object store.</li>
  
<li>The third generation, <tt>s3a:</tt> filesystem. Designed to be a switch in replacement for <tt>s3n:</tt>, this filesystem binding supports larger files and promises higher performance.</li>
</ol>
<p>The specifics of using these filesystems are documented below.</p></div>
<div class="section">
<h3><a name="Warning_1:_Object_Stores_are_not_filesystems."></a>Warning #1: Object Stores are not filesystems.</h3>
<p>Amazon S3 is an example of &#x201c;an object store&#x201d;. In order to achieve scalability and especially high availability, S3 has &#x2014;as many other cloud object stores have done&#x2014; relaxed some of the constraints which classic &#x201c;POSIX&#x201d; filesystems promise.</p>
<p>Specifically</p>

<ol style="list-style-type: decimal">
  
<li>Files that are newly created from the Hadoop Filesystem APIs may not be immediately visible.</li>
  
<li>File delete and update operations may not immediately propagate. Old copies of the file may exist for an indeterminate time period.</li>
  
<li>Directory operations: <tt>delete()</tt> and <tt>rename()</tt> are implemented by recursive file-by-file operations. They take time at least proportional to the number of files, during which time partial updates may be visible. If the operations are interrupted, the filesystem is left in an intermediate state.</li>
</ol></div>
<div class="section">
<h3><a name="Warning_2:_Because_Object_stores_dont_track_modification_times_of_directories"></a>Warning #2: Because Object stores don&#x2019;t track modification times of directories,</h3>
<p>features of Hadoop relying on this can have unexpected behaviour. E.g. the AggregatedLogDeletionService of YARN will not remove the appropriate logfiles.</p>
<p>For further discussion on these topics, please consult <a href="../../../hadoop-project-dist/hadoop-common/filesystem/index.html">The Hadoop FileSystem API Definition</a>.</p></div>
<div class="section">
<h3><a name="Warning_3:_your_AWS_credentials_are_valuable"></a>Warning #3: your AWS credentials are valuable</h3>
<p>Your AWS credentials not only pay for services, they offer read and write access to the data. Anyone with the credentials can not only read your datasets &#x2014;they can delete them.</p>
<p>Do not inadvertently share these credentials through means such as 1. Checking in to SCM any configuration files containing the secrets. 1. Logging them to a console, as they invariably end up being seen. 1. Defining filesystem URIs with the credentials in the URL, such as <tt>s3a://AK0010:secret@landsat/</tt>. They will end up in logs and error messages. 1. Including the secrets in bug reports.</p>
<p>If you do any of these: change your credentials immediately!</p></div>
<div class="section">
<h3><a name="Warning_4:_the_S3_client_provided_by_Amazon_EMR_are_not_from_the_Apache"></a>Warning #4: the S3 client provided by Amazon EMR are not from the Apache</h3>
<p>Software foundation, and are only supported by Amazon.</p>
<p>Specifically: on Amazon EMR, s3a is not supported, and amazon recommend a different filesystem implementation. If you are using Amazon EMR, follow these instructions &#x2014;and be aware that all issues related to S3 integration in EMR can only be addressed by Amazon themselves: please raise your issues with them.</p></div></div>
<div class="section">
<h2><a name="S3"></a>S3</h2>
<p>The <tt>s3://</tt> filesystem is the original S3 store in the Hadoop codebase. It implements an inode-style filesystem atop S3, and was written to provide scaleability when S3 had significant limits on the size of blobs. It is incompatible with any other application&#x2019;s use of data in S3.</p>
<p>It is now deprecated and will be removed in Hadoop 3. Please do not use, and migrate off data which is on it.</p>
<div class="section">
<h3><a name="Dependencies"></a>Dependencies</h3>

<ul>
  
<li><tt>jets3t</tt> jar</li>
  
<li><tt>commons-codec</tt> jar</li>
  
<li><tt>commons-logging</tt> jar</li>
  
<li><tt>httpclient</tt> jar</li>
  
<li><tt>httpcore</tt> jar</li>
  
<li><tt>java-xmlbuilder</tt> jar</li>
</ul></div>
<div class="section">
<h3><a name="Authentication_properties"></a>Authentication properties</h3>

<div class="source">
<div class="source">
<pre>&lt;property&gt;
  &lt;name&gt;fs.s3.awsAccessKeyId&lt;/name&gt;
  &lt;description&gt;AWS access key ID&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3.awsSecretAccessKey&lt;/name&gt;
  &lt;description&gt;AWS secret key&lt;/description&gt;
&lt;/property&gt;
</pre></div></div></div></div>
<div class="section">
<h2><a name="S3N"></a>S3N</h2>
<p>S3N was the first S3 Filesystem client which used &#x201c;native&#x201d; S3 objects, hence the schema <tt>s3n://</tt>.</p>
<div class="section">
<h3><a name="Features"></a>Features</h3>

<ul>
  
<li>Directly reads and writes S3 objects.</li>
  
<li>Compatible with standard S3 clients.</li>
  
<li>Supports partitioned uploads for many-GB objects.</li>
  
<li>Available across all Hadoop 2.x releases.</li>
</ul>
<p>The S3N filesystem client, while widely used, is no longer undergoing active maintenance except for emergency security issues. There are known bugs, especially: it reads to end of a stream when closing a read; this can make <tt>seek()</tt> slow on large files. The reason there has been no attempt to fix this is that every upgrade of the Jets3t library, while fixing some problems, has unintentionally introduced new ones in either the changed Hadoop code, or somewhere in the Jets3t/Httpclient code base. The number of defects remained constant, they merely moved around.</p>
<p>By freezing the Jets3t jar version and avoiding changes to the code, we reduce the risk of making things worse.</p>
<p>The S3A filesystem client can read all files created by S3N. Accordingly it should be used wherever possible.</p></div>
<div class="section">
<h3><a name="Dependencies"></a>Dependencies</h3>

<ul>
  
<li><tt>jets3t</tt> jar</li>
  
<li><tt>commons-codec</tt> jar</li>
  
<li><tt>commons-logging</tt> jar</li>
  
<li><tt>httpclient</tt> jar</li>
  
<li><tt>httpcore</tt> jar</li>
  
<li><tt>java-xmlbuilder</tt> jar</li>
</ul></div>
<div class="section">
<h3><a name="Authentication_properties"></a>Authentication properties</h3>

<div class="source">
<div class="source">
<pre>&lt;property&gt;
  &lt;name&gt;fs.s3n.awsAccessKeyId&lt;/name&gt;
  &lt;description&gt;AWS access key ID&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3n.awsSecretAccessKey&lt;/name&gt;
  &lt;description&gt;AWS secret key&lt;/description&gt;
&lt;/property&gt;
</pre></div></div></div>
<div class="section">
<h3><a name="Other_properties"></a>Other properties</h3>

<div class="source">
<div class="source">
<pre>&lt;property&gt;
  &lt;name&gt;fs.s3.buffer.dir&lt;/name&gt;
  &lt;value&gt;${hadoop.tmp.dir}/s3&lt;/value&gt;
  &lt;description&gt;Determines where on the local filesystem the s3:/s3n: filesystem
  should store files before sending them to S3
  (or after retrieving them from S3).
  &lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3.maxRetries&lt;/name&gt;
  &lt;value&gt;4&lt;/value&gt;
  &lt;description&gt;The maximum number of retries for reading or writing files to
    S3, before we signal failure to the application.
  &lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3.sleepTimeSeconds&lt;/name&gt;
  &lt;value&gt;10&lt;/value&gt;
  &lt;description&gt;The number of seconds to sleep between each S3 retry.
  &lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3n.block.size&lt;/name&gt;
  &lt;value&gt;67108864&lt;/value&gt;
  &lt;description&gt;Block size to use when reading files using the native S3
  filesystem (s3n: URIs).&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3n.multipart.uploads.enabled&lt;/name&gt;
  &lt;value&gt;false&lt;/value&gt;
  &lt;description&gt;Setting this property to true enables multiple uploads to
  native S3 filesystem. When uploading a file, it is split into blocks
  if the size is larger than fs.s3n.multipart.uploads.block.size.
  &lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3n.multipart.uploads.block.size&lt;/name&gt;
  &lt;value&gt;67108864&lt;/value&gt;
  &lt;description&gt;The block size for multipart uploads to native S3 filesystem.
  Default size is 64MB.
  &lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3n.multipart.copy.block.size&lt;/name&gt;
  &lt;value&gt;5368709120&lt;/value&gt;
  &lt;description&gt;The block size for multipart copy in native S3 filesystem.
  Default size is 5GB.
  &lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3n.server-side-encryption-algorithm&lt;/name&gt;
  &lt;value&gt;&lt;/value&gt;
  &lt;description&gt;Specify a server-side encryption algorithm for S3.
  Unset by default, and the only other currently allowable value is AES256.
  &lt;/description&gt;
&lt;/property&gt;
</pre></div></div></div></div>
<div class="section">
<h2><a name="S3A"></a>S3A</h2>
<p>The S3A filesystem client, prefix <tt>s3a://</tt>, is the S3 client undergoing active development and maintenance. While this means that there is a bit of instability of configuration options and behavior, it also means that the code is getting better in terms of reliability, performance, monitoring and other features.</p>
<div class="section">
<h3><a name="Features"></a>Features</h3>

<ul>
  
<li>Directly reads and writes S3 objects.</li>
  
<li>Compatible with standard S3 clients.</li>
  
<li>Can read data created with S3N.</li>
  
<li>Can write data back that is readable by S3N. (Note: excluding encryption).</li>
  
<li>Supports partitioned uploads for many-GB objects.</li>
  
<li>Instrumented with Hadoop metrics.</li>
  
<li>Performance optimized operations, including <tt>seek()</tt> and <tt>readFully()</tt>.</li>
  
<li>Uses Amazon&#x2019;s Java S3 SDK with support for latest S3 features and authentication schemes.</li>
  
<li>Supports authentication via: environment variables, Hadoop configuration properties, the Hadoop key management store and IAM roles.</li>
  
<li>Supports S3 &#x201c;Server Side Encryption&#x201d; for both reading and writing.</li>
  
<li>Supports proxies</li>
  
<li>Test suites includes distcp and suites in downstream projects.</li>
  
<li>Available since Hadoop 2.6; considered production ready in Hadoop 2.7.</li>
  
<li>Actively maintained.</li>
</ul>
<p>S3A is now the recommended client for working with S3 objects. It is also the one where patches for functionality and performance are very welcome.</p></div>
<div class="section">
<h3><a name="Dependencies"></a>Dependencies</h3>

<ul>
  
<li><tt>hadoop-aws</tt> jar.</li>
  
<li><tt>aws-java-sdk-s3</tt> jar.</li>
  
<li><tt>aws-java-sdk-core</tt> jar.</li>
  
<li><tt>aws-java-sdk-kms</tt> jar.</li>
  
<li><tt>joda-time</tt> jar; use version 2.8.1 or later.</li>
  
<li><tt>httpclient</tt> jar.</li>
  
<li>Jackson <tt>jackson-core</tt>, <tt>jackson-annotations</tt>, <tt>jackson-databind</tt> jars.</li>
</ul></div>
<div class="section">
<h3><a name="Authentication_properties"></a>Authentication properties</h3>

<div class="source">
<div class="source">
<pre>&lt;property&gt;
  &lt;name&gt;fs.s3a.access.key&lt;/name&gt;
  &lt;description&gt;AWS access key ID. Omit for IAM role-based or provider-based authentication.&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.secret.key&lt;/name&gt;
  &lt;description&gt;AWS secret key. Omit for IAM role-based or provider-based authentication.&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.aws.credentials.provider&lt;/name&gt;
  &lt;description&gt;
    Class name of a credentials provider that implements
    com.amazonaws.auth.AWSCredentialsProvider.  Omit if using access/secret keys
    or another authentication mechanism.  The specified class must provide an
    accessible constructor accepting java.net.URI and
    org.apache.hadoop.conf.Configuration, or an accessible default constructor.
    Specifying org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider allows
    anonymous access to a publicly accessible S3 bucket without any credentials.
    Please note that allowing anonymous access to an S3 bucket compromises
    security and therefore is unsuitable for most use cases.  It can be useful
    for accessing public data sets without requiring AWS credentials.
  &lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.session.token&lt;/name&gt;
  &lt;description&gt;Session token, when using org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider as the providers.&lt;/description&gt;
&lt;/property&gt;
</pre></div></div>
<div class="section">
<h4><a name="Authentication_methods"></a>Authentication methods</h4>
<p>The standard way to authenticate is with an access key and secret key using the properties above. You can also avoid configuring credentials if the EC2 instances in your cluster are configured with IAM instance profiles that grant the appropriate S3 access.</p>
<p>A temporary set of credentials can also be obtained from Amazon STS; these consist of an access key, a secret key, and a session token. To use these temporary credentials you must include the <tt>aws-java-sdk-sts</tt> JAR in your classpath (consult the POM for the current version) and set the <tt>TemporaryAWSCredentialsProvider</tt> class as the provider. The session key must be set in the property <tt>fs.s3a.session.token</tt> &#x2014;and the access and secret key properties to those of this temporary session.</p>

<div class="source">
<div class="source">
<pre>&lt;property&gt;
  &lt;name&gt;fs.s3a.aws.credentials.provider&lt;/name&gt;
  &lt;value&gt;org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.access.key&lt;/name&gt;
  &lt;value&gt;SESSION-ACCESS-KEY&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.secret.key&lt;/name&gt;
  &lt;value&gt;SESSION-SECRET-KEY&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.session.token&lt;/name&gt;
  &lt;value&gt;SECRET-SESSION-TOKEN&lt;/value&gt;
&lt;/property&gt;
</pre></div></div></div>
<div class="section">
<h4><a name="Protecting_the_AWS_Credentials_in_S3A"></a>Protecting the AWS Credentials in S3A</h4>
<p>To protect the access/secret keys from prying eyes, it is recommended that you use either IAM role-based authentication (such as EC2 instance profile) or the credential provider framework securely storing them and accessing them through configuration. The following describes using the latter for AWS credentials in S3AFileSystem.</p>
<p>For additional reading on the credential provider API see: <a href="../../../hadoop-project-dist/hadoop-common/CredentialProviderAPI.html">Credential Provider API</a>.</p></div>
<div class="section">
<h4><a name="Authenticating_via_environment_variables"></a>Authenticating via environment variables</h4>
<p>S3A supports configuration via <a class="externalLink" href="http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html#cli-environment">the standard AWS environment variables</a>.</p>
<p>The core environment variables are for the access key and associated secret:</p>

<div class="source">
<div class="source">
<pre>export AWS_ACCESS_KEY_ID=my.aws.key
export AWS_SECRET_ACCESS_KEY=my.secret.key
</pre></div></div>
<p>These environment variables can be used to set the authentication credentials instead of properties in the Hadoop configuration. <i>Important:</i> these environment variables are not propagated from client to server when YARN applications are launched. That is: having the AWS environment variables set when an application is launched will not permit the launched application to access S3 resources. The environment variables must (somehow) be set on the hosts/processes where the work is executed.</p>
<div class="section">
<h5><a name="End_to_End_Steps_for_Distcp_and_S3_with_Credential_Providers"></a>End to End Steps for Distcp and S3 with Credential Providers</h5>
<div class="section">
<h6><a name="provision"></a>provision</h6>

<div class="source">
<div class="source">
<pre>% hadoop credential create fs.s3a.access.key -value 123
    -provider localjceks://file/home/lmccay/aws.jceks
</pre></div></div>

<div class="source">
<div class="source">
<pre>% hadoop credential create fs.s3a.secret.key -value 456
    -provider localjceks://file/home/lmccay/aws.jceks
</pre></div></div></div>
<div class="section">
<h6><a name="configure_core-site.xml_or_command_line_system_property"></a>configure core-site.xml or command line system property</h6>

<div class="source">
<div class="source">
<pre>&lt;property&gt;
  &lt;name&gt;hadoop.security.credential.provider.path&lt;/name&gt;
  &lt;value&gt;localjceks://file/home/lmccay/aws.jceks&lt;/value&gt;
  &lt;description&gt;Path to interrogate for protected credentials.&lt;/description&gt;
&lt;/property&gt;
</pre></div></div></div>
<div class="section">
<h6><a name="distcp"></a>distcp</h6>

<div class="source">
<div class="source">
<pre>% hadoop distcp
    [-D hadoop.security.credential.provider.path=localjceks://file/home/lmccay/aws.jceks]
    hdfs://hostname:9001/user/lmccay/007020615 s3a://lmccay/
</pre></div></div>
<p>NOTE: You may optionally add the provider path property to the distcp command line instead of added job specific configuration to a generic core&#xad;site.xml. The square brackets above illustrate this capability.</p></div></div></div></div>
<div class="section">
<h3><a name="Other_properties"></a>Other properties</h3>

<div class="source">
<div class="source">
<pre>&lt;property&gt;
  &lt;name&gt;fs.s3a.connection.maximum&lt;/name&gt;
  &lt;value&gt;15&lt;/value&gt;
  &lt;description&gt;Controls the maximum number of simultaneous connections to S3.&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.connection.ssl.enabled&lt;/name&gt;
  &lt;value&gt;true&lt;/value&gt;
  &lt;description&gt;Enables or disables SSL connections to S3.&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.endpoint&lt;/name&gt;
  &lt;description&gt;AWS S3 endpoint to connect to. An up-to-date list is
    provided in the AWS Documentation: regions and endpoints. Without this
    property, the standard region (s3.amazonaws.com) is assumed.
  &lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.path.style.access&lt;/name&gt;
  &lt;value&gt;false&lt;/value&gt;
  &lt;description&gt;Enable S3 path style access ie disabling the default virtual hosting behaviour.
    Useful for S3A-compliant storage providers as it removes the need to set up DNS for virtual hosting.
  &lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.proxy.host&lt;/name&gt;
  &lt;description&gt;Hostname of the (optional) proxy server for S3 connections.&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.proxy.port&lt;/name&gt;
  &lt;description&gt;Proxy server port. If this property is not set
    but fs.s3a.proxy.host is, port 80 or 443 is assumed (consistent with
    the value of fs.s3a.connection.ssl.enabled).&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.proxy.username&lt;/name&gt;
  &lt;description&gt;Username for authenticating with proxy server.&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.proxy.password&lt;/name&gt;
  &lt;description&gt;Password for authenticating with proxy server.&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.proxy.domain&lt;/name&gt;
  &lt;description&gt;Domain for authenticating with proxy server.&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.proxy.workstation&lt;/name&gt;
  &lt;description&gt;Workstation for authenticating with proxy server.&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.attempts.maximum&lt;/name&gt;
  &lt;value&gt;20&lt;/value&gt;
  &lt;description&gt;How many times we should retry commands on transient errors.&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.connection.establish.timeout&lt;/name&gt;
  &lt;value&gt;5000&lt;/value&gt;
  &lt;description&gt;Socket connection setup timeout in milliseconds.&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.connection.timeout&lt;/name&gt;
  &lt;value&gt;200000&lt;/value&gt;
  &lt;description&gt;Socket connection timeout in milliseconds.&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.paging.maximum&lt;/name&gt;
  &lt;value&gt;5000&lt;/value&gt;
  &lt;description&gt;How many keys to request from S3 when doing
     directory listings at a time.&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.threads.max&lt;/name&gt;
  &lt;value&gt;10&lt;/value&gt;
  &lt;description&gt; Maximum number of concurrent active (part)uploads,
  which each use a thread from the threadpool.&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.threads.keepalivetime&lt;/name&gt;
  &lt;value&gt;60&lt;/value&gt;
  &lt;description&gt;Number of seconds a thread can be idle before being
    terminated.&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.max.total.tasks&lt;/name&gt;
  &lt;value&gt;5&lt;/value&gt;
  &lt;description&gt;Number of (part)uploads allowed to the queue before
  blocking additional uploads.&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.multipart.size&lt;/name&gt;
  &lt;value&gt;104857600&lt;/value&gt;
  &lt;description&gt;How big (in bytes) to split upload or copy operations up into.
  This also controls the partition size in renamed files, as rename() involves
  copying the source file(s)&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.multipart.threshold&lt;/name&gt;
  &lt;value&gt;2147483647&lt;/value&gt;
  &lt;description&gt;Threshold before uploads or copies use parallel multipart operations.&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.multiobjectdelete.enable&lt;/name&gt;
  &lt;value&gt;true&lt;/value&gt;
  &lt;description&gt;When enabled, multiple single-object delete requests are replaced by
    a single 'delete multiple objects'-request, reducing the number of requests.
    Beware: legacy S3-compatible object stores might not support this request.
  &lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.acl.default&lt;/name&gt;
  &lt;description&gt;Set a canned ACL for newly created and copied objects. Value may be private,
     public-read, public-read-write, authenticated-read, log-delivery-write,
     bucket-owner-read, or bucket-owner-full-control.&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.multipart.purge&lt;/name&gt;
  &lt;value&gt;false&lt;/value&gt;
  &lt;description&gt;True if you want to purge existing multipart uploads that may not have been
     completed/aborted correctly&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.multipart.purge.age&lt;/name&gt;
  &lt;value&gt;86400&lt;/value&gt;
  &lt;description&gt;Minimum age in seconds of multipart uploads to purge&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.signing-algorithm&lt;/name&gt;
  &lt;description&gt;Override the default signing algorithm so legacy
    implementations can still be used&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.server-side-encryption-algorithm&lt;/name&gt;
  &lt;description&gt;Specify a server-side encryption algorithm for s3a: file system.
    Unset by default, and the only other currently allowable value is AES256.
  &lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.buffer.dir&lt;/name&gt;
  &lt;value&gt;${hadoop.tmp.dir}/s3a&lt;/value&gt;
  &lt;description&gt;Comma separated list of directories that will be used to buffer file
    uploads to. No effect if fs.s3a.fast.upload is true.&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.block.size&lt;/name&gt;
  &lt;value&gt;33554432&lt;/value&gt;
  &lt;description&gt;Block size to use when reading files using s3a: file system.
  &lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.user.agent.prefix&lt;/name&gt;
  &lt;value&gt;&lt;/value&gt;
  &lt;description&gt;
    Sets a custom value that will be prepended to the User-Agent header sent in
    HTTP requests to the S3 back-end by S3AFileSystem.  The User-Agent header
    always includes the Hadoop version number followed by a string generated by
    the AWS SDK.  An example is &quot;User-Agent: Hadoop 2.8.0, aws-sdk-java/1.10.6&quot;.
    If this optional property is set, then its value is prepended to create a
    customized User-Agent.  For example, if this configuration property was set
    to &quot;MyApp&quot;, then an example of the resulting User-Agent would be
    &quot;User-Agent: MyApp, Hadoop 2.8.0, aws-sdk-java/1.10.6&quot;.
  &lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.impl&lt;/name&gt;
  &lt;value&gt;org.apache.hadoop.fs.s3a.S3AFileSystem&lt;/value&gt;
  &lt;description&gt;The implementation class of the S3A Filesystem&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.AbstractFileSystem.s3a.impl&lt;/name&gt;
  &lt;value&gt;org.apache.hadoop.fs.s3a.S3A&lt;/value&gt;
  &lt;description&gt;The implementation class of the S3A AbstractFileSystem.&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.readahead.range&lt;/name&gt;
  &lt;value&gt;65536&lt;/value&gt;
  &lt;description&gt;Bytes to read ahead during a seek() before closing and
  re-opening the S3 HTTP connection. This option will be overridden if
  any call to setReadahead() is made to an open stream.&lt;/description&gt;
&lt;/property&gt;
</pre></div></div></div>
<div class="section">
<h3><a name="S3AFastOutputStream"></a>S3AFastOutputStream</h3>
<p><b>Warning: NEW in hadoop 2.7. UNSTABLE, EXPERIMENTAL: use at own risk</b></p>

<div class="source">
<div class="source">
<pre>&lt;property&gt;
  &lt;name&gt;fs.s3a.fast.upload&lt;/name&gt;
  &lt;value&gt;false&lt;/value&gt;
  &lt;description&gt;Upload directly from memory instead of buffering to
  disk first. Memory usage and parallelism can be controlled as up to
  fs.s3a.multipart.size memory is consumed for each (part)upload actively
  uploading (fs.s3a.threads.max) or queueing (fs.s3a.max.total.tasks)&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.s3a.fast.buffer.size&lt;/name&gt;
  &lt;value&gt;1048576&lt;/value&gt;
  &lt;description&gt;Size (in bytes) of initial memory buffer allocated for an
  upload. No effect if fs.s3a.fast.upload is false.&lt;/description&gt;
&lt;/property&gt;
</pre></div></div>
<p>Writes are buffered in memory instead of to a file on local disk. This removes the throughput bottleneck of the local disk write and read cycle before starting the actual upload. Furthermore, it allows handling files that are larger than the remaining local disk space.</p>
<p>However, non-trivial memory tuning is needed for optimal results and careless settings could cause memory overflow. Up to <tt>fs.s3a.threads.max</tt> parallel (part)uploads are active. Furthermore, up to <tt>fs.s3a.max.total.tasks</tt> additional part(uploads) can be waiting (and thus memory buffers are created). The memory buffer is uploaded as a single upload if it is not larger than <tt>fs.s3a.multipart.threshold</tt>. Else, a multi-part upload is initiated and parts of size <tt>fs.s3a.multipart.size</tt> are used to protect against overflowing the available memory. These settings should be tuned to the envisioned workflow (some large files, many small ones, &#x2026;) and the physical limitations of the machine and cluster (memory, network bandwidth).</p></div>
<div class="section">
<h3><a name="S3A_Experimental_fadvise_input_policy_support"></a>S3A Experimental &#x201c;fadvise&#x201d; input policy support</h3>
<p><b>Warning: EXPERIMENTAL: behavior may change in future</b></p>
<p>The S3A Filesystem client supports the notion of input policies, similar to that of the Posix <tt>fadvise()</tt> API call. This tunes the behavior of the S3A client to optimise HTTP GET requests for the different use cases.</p>
<div class="section">
<h4><a name="asequential_default"></a>&#x201c;sequential&#x201d; (default)</h4>
<p>Read through the file, possibly with some short forward seeks.</p>
<p>The whole document is requested in a single HTTP request; forward seeks within the readahead range are supported by skipping over the intermediate data.</p>
<p>This is leads to maximum read throughput &#x2014;but with very expensive backward seeks.</p></div>
<div class="section">
<h4><a name="anormal"></a>&#x201c;normal&#x201d;</h4>
<p>This is currently the same as &#x201c;sequential&#x201d;.</p></div>
<div class="section">
<h4><a name="arandom"></a>&#x201c;random&#x201d;</h4>
<p>Optimised for random IO, specifically the Hadoop <tt>PositionedReadable</tt> operations &#x2014;though <tt>seek(offset); read(byte_buffer)</tt> also benefits.</p>
<p>Rather than ask for the whole file, the range of the HTTP request is set to that that of the length of data desired in the <tt>read</tt> operation (Rounded up to the readahead value set in <tt>setReadahead()</tt> if necessary).</p>
<p>By reducing the cost of closing existing HTTP requests, this is highly efficient for file IO accessing a binary file through a series of <tt>PositionedReadable.read()</tt> and <tt>PositionedReadable.readFully()</tt> calls. Sequential reading of a file is expensive, as now many HTTP requests must be made to read through the file.</p>
<p>For operations simply reading through a file: copying, distCp, reading Gzipped or other compressed formats, parsing .csv files, etc, the <tt>sequential</tt> policy is appropriate. This is the default: S3A does not need to be configured.</p>
<p>For the specific case of high-performance random access IO, the <tt>random</tt> policy may be considered. The requirements are:</p>

<ul>
  
<li>Data is read using the <tt>PositionedReadable</tt> API.</li>
  
<li>Long distance (many MB) forward seeks</li>
  
<li>Backward seeks as likely as forward seeks.</li>
  
<li>Little or no use of single character <tt>read()</tt> calls or small <tt>read(buffer)</tt> calls.</li>
  
<li>Applications running close to the S3 data store. That is: in EC2 VMs in the same datacenter as the S3 instance.</li>
</ul>
<p>The desired fadvise policy must be set in the configuration option <tt>fs.s3a.experimental.input.fadvise</tt> when the filesystem instance is created. That is: it can only be set on a per-filesystem basis, not on a per-file-read basis.</p>

<div class="source">
<div class="source">
<pre>&lt;property&gt;
  &lt;name&gt;fs.s3a.experimental.input.fadvise&lt;/name&gt;
  &lt;value&gt;random&lt;/value&gt;
  &lt;description&gt;Policy for reading files.
   Values: 'random', 'sequential' or 'normal'
   &lt;/description&gt;
&lt;/property&gt;
</pre></div></div>
<p><a class="externalLink" href="https://issues.apache.org/jira/browse/HDFS-2744">HDFS-2744</a>, <i>Extend FSDataInputStream to allow fadvise</i> proposes adding a public API to set fadvise policies on input streams. Once implemented, this will become the supported mechanism used for configuring the input IO policy.</p></div></div></div>
<div class="section">
<h2><a name="Troubleshooting_S3A"></a>Troubleshooting S3A</h2>
<p>Common problems working with S3A are</p>

<ol style="list-style-type: decimal">
  
<li>Classpath</li>
  
<li>Authentication</li>
  
<li>S3 Inconsistency side-effects</li>
</ol>
<p>Classpath is usually the first problem. For the S3x filesystem clients, you need the Hadoop-specific filesystem clients, third party S3 client libraries compatible with the Hadoop code, and any dependent libraries compatible with Hadoop and the specific JVM.</p>
<p>The classpath must be set up for the process talking to S3: if this is code running in the Hadoop cluster, the JARs must be on that classpath. That includes <tt>distcp</tt>.</p>
<div class="section">
<h3><a name="ClassNotFoundException:_org.apache.hadoop.fs.s3a.S3AFileSystem"></a><tt>ClassNotFoundException: org.apache.hadoop.fs.s3a.S3AFileSystem</tt></h3>
<p>(or <tt>org.apache.hadoop.fs.s3native.NativeS3FileSystem</tt>, <tt>org.apache.hadoop.fs.s3.S3FileSystem</tt>).</p>
<p>These are the Hadoop classes, found in the <tt>hadoop-aws</tt> JAR. An exception reporting one of these classes is missing means that this JAR is not on the classpath.</p></div>
<div class="section">
<h3><a name="ClassNotFoundException:_com.amazonaws.services.s3.AmazonS3Client"></a><tt>ClassNotFoundException: com.amazonaws.services.s3.AmazonS3Client</tt></h3>
<p>(or other <tt>com.amazonaws</tt> class.) <tt>
This means that one or more of the</tt>aws-*-sdk` JARs are missing. Add them.</p></div>
<div class="section">
<h3><a name="Missing_method_in_AWS_class"></a>Missing method in AWS class</h3>
<p>This can be triggered by incompatibilities between the AWS SDK on the classpath and the version which Hadoop was compiled with.</p>
<p>The AWS SDK JARs change their signature enough between releases that the only way to safely update the AWS SDK version is to recompile Hadoop against the later version.</p>
<p>There&#x2019;s nothing the Hadoop team can do here: if you get this problem, then sorry, but you are on your own. The Hadoop developer team did look at using reflection to bind to the SDK, but there were too many changes between versions for this to work reliably. All it did was postpone version compatibility problems until the specific codepaths were executed at runtime &#x2014;this was actually a backward step in terms of fast detection of compatibility problems.</p></div>
<div class="section">
<h3><a name="Missing_method_in_a_Jackson_class"></a>Missing method in a Jackson class</h3>
<p>This is usually caused by version mismatches between Jackson JARs on the classpath. All Jackson JARs on the classpath <i>must</i> be of the same version.</p></div>
<div class="section">
<h3><a name="Authentication_failure"></a>Authentication failure</h3>
<p>One authentication problem is caused by classpath mismatch; see the joda time issue above.</p>
<p>Otherwise, the general cause is: you have the wrong credentials &#x2014;or somehow the credentials were not readable on the host attempting to read or write the S3 Bucket.</p>
<p>There&#x2019;s not much that Hadoop can do/does for diagnostics here, though enabling debug logging for the package <tt>org.apache.hadoop.fs.s3a</tt> can help.</p>
<p>There is also some logging in the AWS libraries which provide some extra details. In particular, the setting the log <tt>com.amazonaws.auth.AWSCredentialsProviderChain</tt> to log at DEBUG level will mean the invidual reasons for the (chained) authentication clients to fail will be printed.</p>
<p>Otherwise, try to use the AWS command line tools with the same credentials. If you set the environment variables, you can take advantage of S3A&#x2019;s support of environment-variable authentication by attempting to use the <tt>hdfs fs</tt> command to read or write data on S3. That is: comment out the <tt>fs.s3a</tt> secrets and rely on the environment variables.</p>
<p>S3 Frankfurt is a special case. It uses the V4 authentication API.</p></div>
<div class="section">
<h3><a name="Authentication_failures_running_on_Java_8u60"></a>Authentication failures running on Java 8u60+</h3>
<p>A change in the Java 8 JVM broke some of the <tt>toString()</tt> string generation of Joda Time 2.8.0, which stopped the amazon s3 client from being able to generate authentication headers suitable for validation by S3.</p>
<p>Fix: make sure that the version of Joda Time is 2.8.1 or later.</p></div></div>
<div class="section">
<h2><a name="Visible_S3_Inconsistency"></a>Visible S3 Inconsistency</h2>
<p>Amazon S3 is <i>an eventually consistent object store</i>. That is: not a filesystem.</p>
<p>It offers read-after-create consistency: a newly created file is immediately visible. Except, there is a small quirk: a negative GET may be cached, such that even if an object is immediately created, the fact that there &#x201c;wasn&#x2019;t&#x201d; an object is still remembered.</p>
<p>That means the following sequence on its own will be consistent <tt>
touch(path) -&gt; getFileStatus(path)
</tt></p>
<p>But this sequence <i>may</i> be inconsistent.</p>

<div class="source">
<div class="source">
<pre>getFileStatus(path) -&gt; touch(path) -&gt; getFileStatus(path)
</pre></div></div>
<p>A common source of visible inconsistencies is that the S3 metadata database &#x2014;the part of S3 which serves list requests&#x2014; is updated asynchronously. Newly added or deleted files may not be visible in the index, even though direct operations on the object (<tt>HEAD</tt> and <tt>GET</tt>) succeed.</p>
<p>In S3A, that means the <tt>getFileStatus()</tt> and <tt>open()</tt> operations are more likely to be consistent with the state of the object store than any directory list operations (<tt>listStatus()</tt>, <tt>listFiles()</tt>, <tt>listLocatedStatus()</tt>, <tt>listStatusIterator()</tt>).</p>
<div class="section">
<h3><a name="FileNotFoundException_even_though_the_file_was_just_written."></a><tt>FileNotFoundException</tt> even though the file was just written.</h3>
<p>This can be a sign of consistency problems. It may also surface if there is some asynchronous file write operation still in progress in the client: the operation has returned, but the write has not yet completed. While the S3A client code does block during the <tt>close()</tt> operation, we suspect that asynchronous writes may be taking place somewhere in the stack &#x2014;this could explain why parallel tests fail more often than serialized tests.</p></div>
<div class="section">
<h3><a name="File_not_found_in_a_directory_listing_even_though_getFileStatus_finds_it"></a>File not found in a directory listing, even though <tt>getFileStatus()</tt> finds it</h3>
<p>(Similarly: deleted file found in listing, though <tt>getFileStatus()</tt> reports that it is not there)</p>
<p>This is a visible sign of updates to the metadata server lagging behind the state of the underlying filesystem.</p></div>
<div class="section">
<h3><a name="File_not_visiblesaved"></a>File not visible/saved</h3>
<p>The files in an object store are not visible until the write has been completed. In-progress writes are simply saved to a local file/cached in RAM and only uploaded. at the end of a write operation. If a process terminated unexpectedly, or failed to call the <tt>close()</tt> method on an output stream, the pending data will have been lost.</p></div>
<div class="section">
<h3><a name="File_flush_and_hflush_calls_do_not_save_data_to_S3A"></a>File <tt>flush()</tt> and <tt>hflush()</tt> calls do not save data to S3A</h3>
<p>Again, this is due to the fact that the data is cached locally until the <tt>close()</tt> operation. The S3A filesystem cannot be used as a store of data if it is required that the data is persisted durably after every <tt>flush()/hflush()</tt> call. This includes resilient logging, HBase-style journalling and the like. The standard strategy here is to save to HDFS and then copy to S3.</p></div>
<div class="section">
<h3><a name="Other_issues"></a>Other issues</h3>
<p><i>Performance slow</i></p>
<p>S3 is slower to read data than HDFS, even on virtual clusters running on Amazon EC2.</p>

<ul>
  
<li>HDFS replicates data for faster query performance</li>
  
<li>HDFS stores the data on the local hard disks, avoiding network traffic  if the code can be executed on that host. As EC2 hosts often have their  network bandwidth throttled, this can make a tangible difference.</li>
  
<li>HDFS is significantly faster for many &#x201c;metadata&#x201d; operations: listing the contents of a directory, calling <tt>getFileStatus()</tt> on path, creating or deleting directories.</li>
  
<li>On HDFS, Directory renames and deletes are <tt>O(1)</tt> operations. On S3 renaming is a very expensive <tt>O(data)</tt> operation which may fail partway through in which case the final state depends on where the copy+ delete sequence was when it failed. All the objects are copied, then the original set of objects are deleted, so a failure should not lose data &#x2014;it may result in duplicate datasets.</li>
  
<li>Because the write only begins on a <tt>close()</tt> operation, it may be in the final phase of a process where the write starts &#x2014;this can take so long that some things can actually time out.</li>
  
<li>File IO performing many seek calls/positioned read calls will encounter performance problems due to the size of the HTTP requests made. On S3a, the (experimental) fadvise policy &#x201c;random&#x201d; can be set to alleviate this at the expense of sequential read performance and bandwidth.</li>
</ul>
<p>The slow performance of <tt>rename()</tt> surfaces during the commit phase of work, including</p>

<ul>
  
<li>The MapReduce FileOutputCommitter.</li>
  
<li>DistCp&#x2019;s rename after copy operation.</li>
</ul>
<p>Both these operations can be significantly slower when S3 is the destination compared to HDFS or other &#x201c;real&#x201d; filesystem.</p>
<p><i>Improving S3 load-balancing behavior</i></p>
<p>Amazon S3 uses a set of front-end servers to provide access to the underlying data. The choice of which front-end server to use is handled via load-balancing DNS service: when the IP address of an S3 bucket is looked up, the choice of which IP address to return to the client is made based on the the current load of the front-end servers.</p>
<p>Over time, the load across the front-end changes, so those servers considered &#x201c;lightly loaded&#x201d; will change. If the DNS value is cached for any length of time, your application may end up talking to an overloaded server. Or, in the case of failures, trying to talk to a server that is no longer there.</p>
<p>And by default, for historical security reasons in the era of applets, the DNS TTL of a JVM is &#x201c;infinity&#x201d;.</p>
<p>To work with AWS better, set the DNS time-to-live of an application which works with S3 to something lower. See <a class="externalLink" href="http://docs.aws.amazon.com/AWSSdkDocsJava/latest/DeveloperGuide/java-dg-jvm-ttl.html">AWS documentation</a>.</p></div></div>
<div class="section">
<h2><a name="Testing_the_S3_filesystem_clients"></a>Testing the S3 filesystem clients</h2>
<p>Due to eventual consistency, tests may fail without reason. Transient failures, which no longer occur upon rerunning the test, should thus be ignored.</p>
<p>To test the S3* filesystem clients, you need to provide two files which pass in authentication details to the test runner</p>

<ol style="list-style-type: decimal">
  
<li><tt>auth-keys.xml</tt></li>
  
<li><tt>core-site.xml</tt></li>
</ol>
<p>These are both Hadoop XML configuration files, which must be placed into <tt>hadoop-tools/hadoop-aws/src/test/resources</tt>.</p>
<div class="section">
<h3><a name="core-site.xml"></a><tt>core-site.xml</tt></h3>
<p>This file pre-exists and sources the configurations created under <tt>auth-keys.xml</tt>.</p>
<p>For most purposes you will not need to edit this file unless you need to apply a specific, non-default property change during the tests.</p></div>
<div class="section">
<h3><a name="auth-keys.xml"></a><tt>auth-keys.xml</tt></h3>
<p>The presence of this file triggers the testing of the S3 classes.</p>
<p>Without this file, <i>none of the tests in this module will be executed</i></p>
<p>The XML file must contain all the ID/key information needed to connect each of the filesystem clients to the object stores, and a URL for each filesystem for its testing.</p>

<ol style="list-style-type: decimal">
  
<li><tt>test.fs.s3n.name</tt> : the URL of the bucket for S3n tests</li>
  
<li><tt>test.fs.s3a.name</tt> : the URL of the bucket for S3a tests</li>
</ol>
<p>The contents of each bucket will be destroyed during the test process: do not use the bucket for any purpose other than testing. Furthermore, for s3a, all in-progress multi-part uploads to the bucket will be aborted at the start of a test (by forcing <tt>fs.s3a.multipart.purge=true</tt>) to clean up the temporary state of previously failed tests.</p>
<p>Example:</p>

<div class="source">
<div class="source">
<pre>&lt;configuration&gt;

  &lt;property&gt;
    &lt;name&gt;test.fs.s3n.name&lt;/name&gt;
    &lt;value&gt;s3n://test-aws-s3n/&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;name&gt;test.fs.s3a.name&lt;/name&gt;
    &lt;value&gt;s3a://test-aws-s3a/&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;name&gt;fs.s3n.awsAccessKeyId&lt;/name&gt;
    &lt;value&gt;DONOTPCOMMITTHISKEYTOSCM&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;name&gt;fs.s3n.awsSecretAccessKey&lt;/name&gt;
    &lt;value&gt;DONOTEVERSHARETHISSECRETKEY!&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;name&gt;fs.s3a.access.key&lt;/name&gt;
    &lt;description&gt;AWS access key ID. Omit for IAM role-based authentication.&lt;/description&gt;
    &lt;value&gt;DONOTCOMMITTHISKEYTOSCM&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;name&gt;fs.s3a.secret.key&lt;/name&gt;
    &lt;description&gt;AWS secret key. Omit for IAM role-based authentication.&lt;/description&gt;
    &lt;value&gt;DONOTEVERSHARETHISSECRETKEY!&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;name&gt;test.sts.endpoint&lt;/name&gt;
    &lt;description&gt;Specific endpoint to use for STS requests.&lt;/description&gt;
    &lt;value&gt;sts.amazonaws.com&lt;/value&gt;
  &lt;/property&gt;

&lt;/configuration&gt;
</pre></div></div></div>
<div class="section">
<h3><a name="File_contract-test-options.xml"></a>File <tt>contract-test-options.xml</tt></h3>
<p>The file <tt>hadoop-tools/hadoop-aws/src/test/resources/contract-test-options.xml</tt> must be created and configured for the test filesystems.</p>
<p>If a specific file <tt>fs.contract.test.fs.*</tt> test path is not defined for any of the filesystems, those tests will be skipped.</p>
<p>The standard S3 authentication details must also be provided. This can be through copy-and-paste of the <tt>auth-keys.xml</tt> credentials, or it can be through direct XInclude inclusion.</p></div>
<div class="section">
<h3><a name="s3n:"></a>s3n://</h3>
<p>In the file <tt>src/test/resources/contract-test-options.xml</tt>, the filesystem name must be defined in the property <tt>fs.contract.test.fs.s3n</tt>. The standard configuration options to define the S3N authentication details must also be provided.</p>
<p>Example:</p>

<div class="source">
<div class="source">
<pre>  &lt;property&gt;
    &lt;name&gt;fs.contract.test.fs.s3n&lt;/name&gt;
    &lt;value&gt;s3n://test-aws-s3n/&lt;/value&gt;
  &lt;/property&gt;
</pre></div></div></div>
<div class="section">
<h3><a name="s3a:"></a>s3a://</h3>
<p>In the file <tt>src/test/resources/contract-test-options.xml</tt>, the filesystem name must be defined in the property <tt>fs.contract.test.fs.s3a</tt>. The standard configuration options to define the S3N authentication details must also be provided.</p>
<p>Example:</p>

<div class="source">
<div class="source">
<pre>&lt;property&gt;
  &lt;name&gt;fs.contract.test.fs.s3a&lt;/name&gt;
  &lt;value&gt;s3a://test-aws-s3a/&lt;/value&gt;
&lt;/property&gt;
</pre></div></div></div>
<div class="section">
<h3><a name="Complete_example_of_contract-test-options.xml"></a>Complete example of <tt>contract-test-options.xml</tt></h3>

<div class="source">
<div class="source">
<pre>&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;
&lt;!--
  ~ Licensed to the Apache Software Foundation (ASF) under one
  ~  or more contributor license agreements.  See the NOTICE file
  ~  distributed with this work for additional information
  ~  regarding copyright ownership.  The ASF licenses this file
  ~  to you under the Apache License, Version 2.0 (the
  ~  &quot;License&quot;); you may not use this file except in compliance
  ~  with the License.  You may obtain a copy of the License at
  ~
  ~       http://www.apache.org/licenses/LICENSE-2.0
  ~
  ~  Unless required by applicable law or agreed to in writing, software
  ~  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
  ~  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  ~  See the License for the specific language governing permissions and
  ~  limitations under the License.
  --&gt;

&lt;configuration&gt;

  &lt;include xmlns=&quot;http://www.w3.org/2001/XInclude&quot;
    href=&quot;/home/testuser/.ssh/auth-keys.xml&quot;/&gt;

  &lt;property&gt;
    &lt;name&gt;fs.contract.test.fs.s3a&lt;/name&gt;
    &lt;value&gt;s3a://test-aws-s3a/&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
    &lt;name&gt;fs.contract.test.fs.s3n&lt;/name&gt;
    &lt;value&gt;s3n://test-aws-s3n/&lt;/value&gt;
  &lt;/property&gt;

&lt;/configuration&gt;
</pre></div></div>
<p>This example pulls in the <tt>~/.ssh/auth-keys.xml</tt> file for the credentials. This provides one single place to keep the keys up to date &#x2014;and means that the file <tt>contract-test-options.xml</tt> does not contain any secret credentials itself. As the auth keys XML file is kept out of the source code tree, it is not going to get accidentally committed.</p></div>
<div class="section">
<h3><a name="Running_Tests_against_non-AWS_storage_infrastructures"></a>Running Tests against non-AWS storage infrastructures</h3></div>
<div class="section">
<h3><a name="S3A_session_tests"></a>S3A session tests</h3>
<p>The test <tt>TestS3ATemporaryCredentials</tt> requests a set of temporary credentials from the STS service, then uses them to authenticate with S3.</p>
<p>If an S3 implementation does not support STS, then the functional test cases must be disabled:</p>

<div class="source">
<div class="source">
<pre>    &lt;property&gt;
      &lt;name&gt;test.fs.s3a.sts.enabled&lt;/name&gt;
      &lt;value&gt;false&lt;/value&gt;
    &lt;/property&gt;
</pre></div></div>
<p>These tests reqest a temporary set of credentials from the STS service endpoint. An alternate endpoint may be defined in <tt>test.fs.s3a.sts.endpoint</tt>.</p>

<div class="source">
<div class="source">
<pre>    &lt;property&gt;
      &lt;name&gt;test.fs.s3a.sts.endpoint&lt;/name&gt;
      &lt;value&gt;https://sts.example.org/&lt;/value&gt;
    &lt;/property&gt;
</pre></div></div>
<p>The default is &quot;&#x201c;; meaning &#x201d;use the amazon default value&quot;.</p>
<div class="section">
<h4><a name="CSV_Data_source"></a>CSV Data source</h4>
<p>The <tt>TestS3AInputStreamPerformance</tt> tests require read access to a multi-MB text file. The default file for these tests is one published by amazon, <a class="externalLink" href="http://landsat-pds.s3.amazonaws.com/scene_list.gz">s3a://landsat-pds.s3.amazonaws.com/scene_list.gz</a>. This is a gzipped CSV index of other files which amazon serves for open use.</p>
<p>The path to this object is set in the option <tt>fs.s3a.scale.test.csvfile</tt>:</p>

<div class="source">
<div class="source">
<pre>&lt;property&gt;
  &lt;name&gt;fs.s3a.scale.test.csvfile&lt;/name&gt;
  &lt;value&gt;s3a://landsat-pds/scene_list.gz&lt;/value&gt;
&lt;/property&gt;
</pre></div></div>

<ol style="list-style-type: decimal">
  
<li>If the option is not overridden, the default value is used. This is hosted in Amazon&#x2019;s US-east datacenter.</li>
  
<li>If the property is empty, tests which require it will be skipped.</li>
  
<li>If the data cannot be read for any reason then the test will fail.</li>
  
<li>If the property is set to a different path, then that data must be readable and &#x201c;sufficiently&#x201d; large.</li>
</ol>
<p>To test on different S3 endpoints, or alternate infrastructures supporting the same APIs, the option <tt>fs.s3a.scale.test.csvfile</tt> must therefore be set to &#x201c; &#x201d;, or an object of at least 10MB is uploaded to the object store, and the <tt>fs.s3a.scale.test.csvfile</tt> option set to its path.</p>

<div class="source">
<div class="source">
<pre>  &lt;property&gt;
    &lt;name&gt;fs.s3a.scale.test.csvfile&lt;/name&gt;
    &lt;value&gt; &lt;/value&gt;
  &lt;/property&gt;
</pre></div></div></div>
<div class="section">
<h4><a name="Scale_test_operation_count"></a>Scale test operation count</h4>
<p>Some scale tests perform multiple operations (such as creating many directories).</p>
<p>The exact number of operations to perform is configurable in the option <tt>scale.test.operation.count</tt></p>

<div class="source">
<div class="source">
<pre>  &lt;property&gt;
    &lt;name&gt;scale.test.operation.count&lt;/name&gt;
    &lt;value&gt;10&lt;/value&gt;
  &lt;/property&gt;
</pre></div></div>
<p>Larger values generate more load, and are recommended when testing locally, or in batch runs.</p>
<p>Smaller values results in faster test runs, especially when the object store is a long way away.</p>
<p>Operations which work on directories have a separate option: this controls the width and depth of tests creating recursive directories. Larger values create exponentially more directories, with consequent performance impact.</p>

<div class="source">
<div class="source">
<pre>  &lt;property&gt;
    &lt;name&gt;scale.test.directory.count&lt;/name&gt;
    &lt;value&gt;2&lt;/value&gt;
  &lt;/property&gt;
</pre></div></div>
<p>DistCp tests targeting S3A support a configurable file size. The default is 10 MB, but the configuration value is expressed in KB so that it can be tuned smaller to achieve faster test runs.</p>

<div class="source">
<div class="source">
<pre>  &lt;property&gt;
    &lt;name&gt;scale.test.distcp.file.size.kb&lt;/name&gt;
    &lt;value&gt;10240&lt;/value&gt;
  &lt;/property&gt;
</pre></div></div></div></div>
<div class="section">
<h3><a name="Running_the_Tests"></a>Running the Tests</h3>
<p>After completing the configuration, execute the test run through Maven.</p>

<div class="source">
<div class="source">
<pre>mvn clean test
</pre></div></div>
<p>It&#x2019;s also possible to execute multiple test suites in parallel by enabling the <tt>parallel-tests</tt> Maven profile. The tests spend most of their time blocked on network I/O with the S3 service, so running in parallel tends to complete full test runs faster.</p>

<div class="source">
<div class="source">
<pre>mvn -Pparallel-tests clean test
</pre></div></div>
<p>Some tests must run with exclusive access to the S3 bucket, so even with the <tt>parallel-tests</tt> profile enabled, several test suites will run in serial in a separate Maven execution step after the parallel tests.</p>
<p>By default, the <tt>parallel-tests</tt> profile runs 4 test suites concurrently. This can be tuned by passing the <tt>testsThreadCount</tt> argument.</p>

<div class="source">
<div class="source">
<pre>mvn -Pparallel-tests -DtestsThreadCount=8 clean test
</pre></div></div></div>
<div class="section">
<h3><a name="Testing_against_non_AWS_S3_endpoints."></a>Testing against non AWS S3 endpoints.</h3>
<p>The S3A filesystem is designed to work with storage endpoints which implement the S3 protocols to the extent that the amazon S3 SDK is capable of talking to it. We encourage testing against other filesystems and submissions of patches which address issues. In particular, we encourage testing of Hadoop release candidates, as these third-party endpoints get even less testing than the S3 endpoint itself.</p>
<p><b>Disabling the encryption tests</b></p>
<p>If the endpoint doesn&#x2019;t support server-side-encryption, these will fail</p>

<div class="source">
<div class="source">
<pre>  &lt;property&gt;
    &lt;name&gt;test.fs.s3a.encryption.enabled&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
  &lt;/property&gt;
</pre></div></div>
<p>Encryption is only used for those specific test suites with <tt>Encryption</tt> in their classname.</p></div></div>
      </div>
    </div>
    <div class="clear">
      <hr/>
    </div>
    <div id="footer">
      <div class="xright">
        &#169;            2016
              Apache Software Foundation
            
                          - <a href="http://maven.apache.org/privacy-policy.html">Privacy Policy</a>.
        Apache Maven, Maven, Apache, the Apache feather logo, and the Apache Maven project logos are trademarks of The Apache Software Foundation.
      </div>
      <div class="clear">
        <hr/>
      </div>
    </div>
  </body>
</html>
